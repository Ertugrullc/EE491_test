{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "ZERO_KEY = \"<ZERO>\"\n",
    "\n",
    "def hotvec_dict(word_dict):\n",
    "    word_dict = dict(filter(lambda elem: elem[1] > THRESHOLD, word_dict.items()))\n",
    "    vec_mat = np.eye(len(word_dict))\n",
    "    return {k: vec_mat[list(word_dict.keys()).index(k)] for k in word_dict}, word_dict\n",
    "\n",
    "hotvec, word_dict = hotvec_dict(word_dict)\n",
    "hotvec.update({ZERO_KEY:np.zeros([1,len(word_dict)],dtype=float)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X train\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "THRESHOLD_TXT_WORD = 10\n",
    "BASE_PATH = \"42bin_haber/news\"\n",
    "TARGET_PATH = \"cleared_samples\"\n",
    "SAMPLE_FILE_NAME = \"all_words.txt\"\n",
    "\n",
    "PATHS_OF_CLASSES = [os.path.join(BASE_PATH, file) for file in os.listdir(BASE_PATH)]\n",
    "\n",
    "def get_training_data(hotvec, PATHS_OF_CLASSES, word_dict):\n",
    "    \n",
    "    vec_mat_y = np.eye(len(PATHS_OF_CLASSES))\n",
    "    dict_y = {basename(path): vec_mat_y[PATHS_OF_CLASSES.index(path)] for path in PATHS_OF_CLASSES}\n",
    "    \n",
    "    training_data = []\n",
    "    ##\n",
    "    folder = 0\n",
    "    ##\n",
    "    for path in PATHS_OF_CLASSES:\n",
    "        class_name = basename(path)\n",
    "        y_data = dict_y[class_name]\n",
    "        \n",
    "        ##\n",
    "        fil = 0\n",
    "        ##\n",
    "        for file in os.listdir(path):\n",
    "            fil+=1\n",
    "            print(class_name,\":\",fil)\n",
    "            if file.endswith('.txt'):\n",
    "                with open(os.path.join(path, file),'r',encoding=\"utf-8\",errors='ignore') as f:\n",
    "                    text = f.read()\n",
    "                    word_list = clear_sample(text, word_dict)   ## you can change clear type by writing new function\n",
    "                    X_data = [hotvec[w] for w in word_list]\n",
    "                    training_data.append([X_data, y_data])\n",
    "    \n",
    "    return training_data\n",
    "                    \n",
    "        \n",
    "def clear_sample(text, word_dict):\n",
    "    remov = set(stopwords.words(\"turkish\"))\n",
    "    words = word_tokenize(text)\n",
    "    result_words = []\n",
    "    for w in words:\n",
    "        temp = w.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).split()\n",
    "        result_words.extend(t for t in temp)\n",
    "        \n",
    "    words = [word.lower() for word in result_words]\n",
    "    words = [w.strip() for w in words if '\\n' or '' != w]\n",
    "    words = [w for w in words if w not in remov]\n",
    "    words = [x for x in words if not any(c.isdigit() for c in x)]\n",
    "    words = filter (lambda word: len (word) > 2, words)\n",
    "    \n",
    "    w_list = []\n",
    "    \n",
    "    for w in words:\n",
    "        if w in word_dict:\n",
    "            w_list.append(w)\n",
    "        if len(w_list) == THRESHOLD_TXT_WORD:\n",
    "            break\n",
    "    \n",
    "    while len(w_list) < THRESHOLD_TXT_WORD:\n",
    "        w_list.append(ZERO_KEY)\n",
    "        \n",
    "    return w_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = get_training_data(hotvec, PATHS_OF_CLASSES, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(training_data)\n",
    "np.save(\"training_data_shuffled.npy\", training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
